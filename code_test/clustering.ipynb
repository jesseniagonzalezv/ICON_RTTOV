{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3388fb95-5994-4437-b659-b1b55c5116a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## check the clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab64fdd8-dbac-4f6d-8fd9-ffb4bacfc964",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'icecream'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01margparse\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01micecream\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ic\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m preprocessing\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecomposition\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PCA\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'icecream'"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from pyhdf.SD import SD, SDC\n",
    "import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "from icecream import ic\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "\n",
    "def pair_plot(name_plot,X,n_components,path_output):    \n",
    "    Bandnames = {str(i): f\"Band {i+1}\" for i in range(n_components)}\n",
    "\n",
    "    a = sns.pairplot(pd.DataFrame(X[:,:n_components],\n",
    "                        columns = Bandnames),\n",
    "                         diag_kind='kde',plot_kws={\"s\": 3})\n",
    "    a.fig.suptitle(name_plot, y=1.00)\n",
    "    plt.tight_layout()\n",
    "    a.savefig(\"{}/{}.png\".format(path_output,name_plot)) \n",
    "    \n",
    "    plt.close()\n",
    "    \n",
    "def cov_eigval_numpy(X_scaled):\n",
    "        # Covariance\n",
    "    np.set_printoptions(precision=3)\n",
    "    cov = np.cov(X_scaled.transpose())\n",
    "\n",
    "        # Eigen Values\n",
    "    EigVal, EigVec = np.linalg.eig(cov)\n",
    "\n",
    "    print(\"Eigenvalues:\\n\\n\", EigVal,\"\\n\")\n",
    "    print(\"Percentage of Variance Explained by Each Component: \\n\", EigVal/sum(EigVal))\n",
    "        \n",
    "        # Ordering Eigen values and vectors\n",
    "    order = EigVal.argsort()[::-1]\n",
    "    EigVal = EigVal[order]\n",
    "    EigVec = EigVec[:,order]\n",
    "        \n",
    "        #Projecting data on Eigen vector directions resulting to Principal Components \n",
    "    PC = np.matmul(X_scaled,EigVec)   #cross product\n",
    "    \n",
    "    tot = sum(EigVal)  #https://medium.com/luca-chuangs-bapm-notes/principal-component-analysis-pca-using-python-scikit-learn-48c4c13e49af\n",
    "    var_exp = [(i / tot) for i in sorted(EigVal, reverse=True)]\n",
    "    cum_var_exp = np.cumsum(var_exp)    \n",
    "    ic(cum_var_exp)\n",
    "        \n",
    "    return PC,var_exp,cum_var_exp\n",
    "\n",
    "def variance_numpy_plot(name_plot,var_exp,cum_var_exp,n_components, path_output): \n",
    "    fig, ax = plt.subplots()\n",
    "    plt.bar(range(1,n_components+1), var_exp, alpha=0.5, align='center',\n",
    "            label='Individual explained variance')\n",
    "    plt.step(range(1,n_components+1), cum_var_exp, where='mid',\n",
    "             label='Cumulative explained variance',\n",
    "             color='red')\n",
    "    plt.ylabel('Explained variance ratio')\n",
    "    plt.xlabel('Principal component index')\n",
    "    fig.savefig(\"{}/{}.png\".format(path_output,name_plot)) \n",
    "\n",
    "    plt.close()\n",
    "\n",
    "def variance_sklearn_plot(name_plot,pca,n_components, path_output):\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    plt.bar(range(1,n_components+1), pca.explained_variance_ratio_,\n",
    "            alpha=0.5,\n",
    "            align='center')\n",
    "    plt.step(range(1,n_components+1), np.cumsum(pca.explained_variance_ratio_),\n",
    "             where='mid',\n",
    "             color='red')\n",
    "    plt.ylabel('Explained variance ratio')\n",
    "    plt.xlabel('Principal Components')\n",
    "    fig.savefig(\"{}/{}.png\".format(path_output,name_plot)) \n",
    "    print(\"{}/{}.png\".format(path_output,name_plot))\n",
    "    plt.close()\n",
    "    \n",
    "    \n",
    "def PCA_calculation(X_scaled,name_plot,n_pca,path_output):\n",
    "    #     ###########numpy PCA####################################################\n",
    "    PC,var_exp,cum_var_exp=cov_eigval_numpy(X_scaled) #it allows to show all the PCs\n",
    "    #     pair_plot(\"Pair plot of PCs\",PC,n_bands,path_output)    \n",
    "    #     variance_numpy_plot(name_plot,var_exp,cum_var_exp,n_components, path_output)\n",
    "    #     ###########end numpy PCA####################################################\n",
    "\n",
    "    # fit_transform() is used to calculate the PCAs from training data\n",
    "    pca = PCA(n_components=n_pca) #PCA(n_components=24)\n",
    "    pca.fit(X_scaled)\n",
    "    X_reduced = pca.transform(X_scaled)\n",
    "    # to get the fit statistics (variance explained per component)\n",
    "    #print(\"sklearn var:\\n\", pca.explained_variance_ratio_) #no ordered\n",
    "    print(f\" sum of explained variance ratios of the PCs : {n_pca,sum(pca.explained_variance_ratio_)}\")\n",
    "    print(\"explained variance:\",np.cumsum(pca.explained_variance_ratio_))  ########????????????????? check here what is the PC\n",
    "\n",
    "    variance_sklearn_plot(name_plot,pca,n_pca, path_output) \n",
    "    \n",
    "    # pk.dump(PC, open(\"pca_target.pkl\",\"wb\"))\n",
    "\n",
    "    return X_reduced, pca #PC = X_reduced\n",
    "\n",
    "def convert_3D(PC, img_shape,n_bands): \n",
    "    #https://towardsdatascience.com/principal-component-analysis-in-depth-understanding-through-image-visualization-892922f77d9f\n",
    "    import cv2\n",
    "    \n",
    "    ic(img_shape, np.shape(PC))\n",
    "    #PC_2d = np.zeros((img_shape[0],img_shape[1],n_bands))\n",
    "    PC_2d = np.zeros((628,img_shape[1],n_bands))  #REVISAR PARA QUE CUADRE AL ELIMINAR LOS NANS CREO Q SON DE LA PARTE BAJA\n",
    "\n",
    "    for i in range(n_bands):\n",
    "        PC_2d[:,:,i] = PC[:,i].reshape(-1,img_shape[1])\n",
    "\n",
    "    # normalizing between 0 to 255\n",
    "    #PC_2d_Norm = np.zeros((img_shape[0],img_shape[1],n_bands))\n",
    "    PC_2d_Norm = np.zeros((628,img_shape[1],n_bands))#REVISAR PARA QUE CUADRE AL ELIMINAR LOS NANS CREO Q SON DE LA PARTE BAJA\n",
    "\n",
    "    for i in range(n_bands):\n",
    "        PC_2d_Norm[:,:,i] = cv2.normalize(PC_2d[:,:,i], np.zeros(img_shape),0,255 ,cv2.NORM_MINMAX)\n",
    "                     \n",
    "                     \n",
    "    return PC_2d_Norm\n",
    "                     \n",
    "                     \n",
    "def plot_PC(PC_2d_Norm, n_bands, path_output):\n",
    "    \n",
    "    fig,axes = plt.subplots(6,6,figsize=(50,23),sharex='all',sharey='all')\n",
    "    fig.subplots_adjust(wspace=0.1, hspace=0.15)\n",
    "    fig.suptitle('Intensities of Principal Components ', fontsize=30)\n",
    "\n",
    "    axes = axes.ravel()\n",
    "    for i in range(n_bands):\n",
    "        axes[i].imshow(PC_2d_Norm[:,:,i],cmap='gray', vmin=0, vmax=255,  origin='lower')\n",
    "        axes[i].set_title('PC '+str(i+1),fontsize=25)\n",
    "        axes[i].axis('off')\n",
    "    #fig.delaxes(axes[-1])                 \n",
    "    fig.savefig(\"{}/Intensities PC.png\".format(path_output)) \n",
    "    plt.close() \n",
    "                     \n",
    "                     \n",
    "##not used because i deleted some nan points,only if the data is completed##################\n",
    "def reconstruction_img(X_reduced,pca,img_shape,n_bands):\n",
    "    X_img_reduced = np.zeros((628,img_shape[1],n_bands)) #REVISAR PARA QUE CUADRE AL ELIMINAR LOS NANS CREO Q SON DE LA PARTE BAJA\n",
    "\n",
    "    #X_img_reduced = np.zeros((img_shape[0],img_shape[1],n_bands))\n",
    "    ic(X_reduced.shape)\n",
    "\n",
    "    X_inv_pca = pca.inverse_transform(X_reduced)\n",
    "    X_inversed_scaler = scaler.inverse_transform(X_inv_pca)\n",
    "    ic(X_inversed_scaler.shape)\n",
    "\n",
    "    for i in range(n_bands):\n",
    "        X_img_reduced[:,:,i] = X_inversed_pca[:,i].reshape(-1,img_shape[1])\n",
    "    ###only to visualize the image reduced show is really close to the original\n",
    "    print(\"shape image reconstructed\",X_img_reduced.shape)\n",
    "    return X_img_reduced\n",
    "\n",
    "\n",
    "def dataframe_csv(variable, colum, path_output, name_file):\n",
    "  ### input (a,b,c) a will be the columns of the dataframe\n",
    "  # datafram  row = b*c, colum = a  \n",
    "    print('dataframe', np.shape(colum), np.shape(variable))\n",
    "    X_flated = variable.reshape(-1,variable.shape[2]) # #np.stack(X_list, axis=-1)\n",
    "    \n",
    "    print(np.shape(X_flated))\n",
    "    df=pd.DataFrame(X_flated) \n",
    "    \n",
    "    \n",
    "    for i in range(len(colum)):\n",
    "        count_nan = df[i].isnull().sum()\n",
    "        print ('In band {} values NaN: {}'.format(colum[i], count_nan))  \n",
    "\n",
    "    \n",
    "    df_after_drop=df.dropna( how = 'any' ) # subset = [1],‘any’ : If any NA values are present, drop that row or column.  NOSE COMO RECONSTRUIR revisar si esto es los ultimos de la parte baja puedo poner simplemente 0 valor REVISAR\n",
    "    \n",
    "    \n",
    "    pd.set_option('display.float_format', lambda x: '%.1f' % x)\n",
    "    df.columns= colum\n",
    "    \n",
    "    df.describe().to_csv(\"{}/{}.csv\".format(path_output, name_file))     \n",
    "    print(\"ok dataframe\")\n",
    "    \n",
    "    return df_after_drop\n",
    "        \n",
    "    #df_after_drop= df.drop([8,9,10,11,12,13,14,15,16,17], axis=1)\n",
    "    #df_after_drop=df_after_drop.dropna( subset = [1,5,7,18,37], how = 'any' )\n",
    "    #ic(df_after_drop.count())  \n",
    "    \n",
    "def plot_image_PC(PC_2d_Norm, image_3bands,  path_output):\n",
    "\n",
    "    #img=img.transpose(1,2,0)\n",
    "    print(np.shape(image_3bands),np.min(image_3bands),np.max(image_3bands))\n",
    "\n",
    "    #img2 = (img[:,:,:3].astype(np.float32))/np.max(img)\n",
    "    #inp = (img*255).astype(np.uint8)\n",
    "    fig = plt.figure(figsize=(50,30))  \n",
    "    plt.subplots_adjust(hspace=0.2, wspace=0.2)\n",
    "    ax1=plt.subplot(131)\n",
    "   # ax1.imshow(np.fliplr(image_3bands),  origin='lower')#,vmin=100,vmax=1500)\n",
    "    ax1.imshow((image_3bands),  origin='lower')#,vmin=100,vmax=1500)\n",
    "\n",
    "    ax2=plt.subplot(132)\n",
    "    ax2.imshow(PC_2d_Norm[:,:,:3][:,:,[0,2,1]].astype(int),  origin='lower') #aca creo que xq rgb esta en otro orden \n",
    "    ax2.axis('off')\n",
    "    \n",
    "   # #ax2=plt.subplot(132)\n",
    "    ##ax2.imshow(mask_overlay(img2, mask))\n",
    "    fig.savefig(\"{}/image_rgb_3PC.png\".format(path_output)) \n",
    "    plt.close()\n",
    "\n",
    "def plot_image_rgb(image_3bands,  path_output):\n",
    "\n",
    "    print(np.shape(image_3bands),np.min(image_3bands),np.max(image_3bands))\n",
    "\n",
    "    fig = plt.figure(figsize=(20,15))  \n",
    "    plt.subplots_adjust(hspace=0.2, wspace=0.2)\n",
    "    ax1=plt.subplot(131)\n",
    "    ax1.imshow(image_3bands,  origin='lower')# 1,4,3 =0,2,1\n",
    "\n",
    "    plt.title('RGB')\n",
    "\n",
    "    fig.savefig(\"{}/image_rgb_T12.png\".format(path_output)) \n",
    "    plt.close()\n",
    "    \n",
    "from sklearn.cluster import KMeans\n",
    "    \n",
    "def get_kmeans(nbands, imagery, path_output):\n",
    "    #import natsort\n",
    "\n",
    "    # create an empty array in which each column will hold a flattened band\n",
    "    flat_data = np.empty((imagery.shape[0]*imagery.shape[1], nbands)) #x,y\n",
    "\n",
    "    # loop through each band in the image and add to the data array\n",
    "    n_PC = 6 #2 #3 test JQ\n",
    "    for i in range(n_PC): #nbands):\n",
    "        band = imagery[:,:,i] #ch in the last 2\n",
    "        flat_data[:, i-1] = band.flatten()\n",
    "\n",
    "    # set up the kmeans classification by specifying the number of clusters \n",
    "    n_cluster = 5\n",
    "    km = KMeans(n_clusters = n_cluster)\n",
    "    # begin iteratively computing the position of the two clusters\n",
    "    km.fit(flat_data)\n",
    "\n",
    "    # use the sklearn kmeans .predict method to assign all the pixels of an image to a unique cluster\n",
    "    flat_predictions = km.predict(flat_data)\n",
    "\n",
    "    # rehsape the flattened precition array into an MxN prediction mask\n",
    "    prediction_mask = flat_predictions.reshape((imagery.shape[0], imagery.shape[1])) #x,y\n",
    "    ic(\"prediction_mask\", np.shape(prediction_mask))\n",
    "    #plot the imagery and the prediction mask for comparison\n",
    "\n",
    "    fig = plt.figure(figsize=(20,15))  \n",
    "\n",
    "    # plt.imshow(imagery[0,:,:])\n",
    "    # plt.title(\"Imagery\")\n",
    "    # plt.axis('off')\n",
    "    # plt.close()\n",
    "\n",
    "    plt.imshow(prediction_mask,  origin='lower') \n",
    "    plt.title('kmeans predictions')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    fig.savefig(\"{}/kmeans predictions_{}PC_{}cluster.png\".format(path_output,n_PC,n_cluster)) \n",
    "\n",
    "    plt.close()\n",
    "\n",
    "def n_clusters(nbands, imagery, path_output):\n",
    "    # create an empty array in which each column will hold a flattened band\n",
    "    flat_data = np.empty((imagery.shape[0]*imagery.shape[1], nbands)) #x,y\n",
    "\n",
    "    # loop through each band in the image and add to the data array\n",
    "    n_PC = 2 #3 test JQ\n",
    "    for i in range(n_PC): #nbands):\n",
    "        band = imagery[:,:,i] #ch in the last 2\n",
    "        flat_data[:, i-1] = band.flatten()\n",
    "        \n",
    "    data_transformed = flat_data\n",
    "    \n",
    "    Sum_of_squared_distances = []\n",
    "    K = range(1,15)\n",
    "    for k in K:\n",
    "        km = KMeans(n_clusters=k)\n",
    "        km = km.fit(data_transformed)\n",
    "        Sum_of_squared_distances.append(km.inertia_)\n",
    "        \n",
    "    fig = plt.figure(figsize=(20,15))  \n",
    "\n",
    "    plt.plot(K, Sum_of_squared_distances, 'bx-')\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('Sum_of_squared_distances')\n",
    "    plt.title('Elbow Method For Optimal k')\n",
    "    fig.savefig(\"{}/Elbow Method For Optimal.png\".format(path_output)) \n",
    "\n",
    "    plt.close()\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b09ea5-a7e9-4247-8a79-1b9bda9d2721",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (based on the module python3/2022.01)",
   "language": "python",
   "name": "python3_2022_01"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
